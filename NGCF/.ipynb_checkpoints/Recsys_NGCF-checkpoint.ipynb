{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bed5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import multiprocessing\n",
    "from data.worker import worker\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e43e51",
   "metadata": {},
   "source": [
    "## 1. 데이터 가공\n",
    "userId_problemId.csv 파일 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346bf463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>problemId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sos0911</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sos0911</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sos0911</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sos0911</td>\n",
       "      <td>1003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sos0911</td>\n",
       "      <td>1005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  problemId\n",
       "0  sos0911       1000\n",
       "1  sos0911       1001\n",
       "2  sos0911       1002\n",
       "3  sos0911       1003\n",
       "4  sos0911       1005"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/userId_problemId.csv\").loc[:,['userId', 'problemId']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535b09ab",
   "metadata": {},
   "source": [
    "### 1) userId remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdab6deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_user():\n",
    "    user_problem_df = pd.read_csv(\"./data/userId_problemId.csv\").loc[:, ['userId', 'problemId']]\n",
    "    user_df = pd.DataFrame({\"user_id\": [], \"remap_id\": []})\n",
    "    unique_sorted_user = sorted(user_problem_df['userId'].unique().tolist())\n",
    "    for i in range(len(unique_sorted_user)):\n",
    "        new_user_df = pd.DataFrame({\"user_id\": [unique_sorted_user[i]], \"remap_id\": [i]})\n",
    "        user_df = pd.concat((user_df, new_user_df))\n",
    "        \n",
    "    user_df.to_csv(\"./data/user_list.csv\", index=False)\n",
    "    print('user_list file saved successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17b0d93",
   "metadata": {},
   "source": [
    "### 2) problemId remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "427ed8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_problem():\n",
    "    user_problem_df = pd.read_csv(\"./data/userId_problemId.csv\").loc[:, ['userId', 'problemId']]\n",
    "    problem_df = pd.DataFrame({\"problem_id\": [], \"remap_id\": []})\n",
    "    unique_sorted_problem = sorted(user_problem_df['problemId'].unique().tolist())\n",
    "    for i in range(len(unique_sorted_problem)):\n",
    "        new_problem_df = pd.DataFrame({\"problem_id\": [unique_sorted_problem[i]], \"remap_id\": [i]})\n",
    "        problem_df = pd.concat((problem_df, new_problem_df))\n",
    "    \n",
    "    problem_df.to_csv(\"./data/problem_list.csv\", index=False)\n",
    "    print('problem_list file saved successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40825454",
   "metadata": {},
   "source": [
    "### 3) Convert userId, problemId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ba9093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_user_problem():\n",
    "    user_problem_df = pd.read_csv(\"./data/userId_problemId.csv\").loc[:, ['userId', 'problemId']]\n",
    "    user_df = pd.read_csv(\"./data/user_list.csv\")\n",
    "    problem_df = pd.read_csv(\"./data/problem_list.csv\")\n",
    "\n",
    "    manager = multiprocessing.Manager()\n",
    "    user_problem_remap_df = pd.DataFrame({'userId': [], 'problemId': []})\n",
    "\n",
    "    for i in range(0, user_problem_df['userId'].size, 10000):\n",
    "        clear_output(wait=True)\n",
    "        print('Loading: [{}]'.format('-' * (i // 10000) + '>' + '-' * (83 - i // 10000)))\n",
    "        \n",
    "        return_dict = manager.dict()\n",
    "        jobs = []\n",
    "        \n",
    "        for j in range(4):\n",
    "            p = multiprocessing.Process(target=worker, args=(j, i + 2500 * j, user_df, problem_df, user_problem_df[i + 2500 * j:i + min(user_problem_df['userId'].size, 2500*(j+1))], return_dict))\n",
    "            jobs.append(p)\n",
    "            p.start()\n",
    "            if i + 2500 * (j+1) >= user_problem_df['userId'].size: break\n",
    "        for proc in jobs:\n",
    "            proc.join()\n",
    "            proc.close()\n",
    "        for j in range(len(return_dict.keys())):\n",
    "            user_problem_remap_df = pd.concat((user_problem_remap_df, return_dict[j]))\n",
    "    user_problem_remap_df.to_csv(\"./data/userId_problemId_remap.csv\", index=False)\n",
    "    print('userId_problemId_remap file saved successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a176afaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_list file saved successfully\n"
     ]
    }
   ],
   "source": [
    "remap_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2e67b59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem_list file saved successfully\n"
     ]
    }
   ],
   "source": [
    "remap_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b73ef",
   "metadata": {},
   "source": [
    "multiprocessing을 사용하여 30분 이상 걸리던 작업을 13분으로 단축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "295ccb55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: [----------------------------------------------------------------------------------->]\n",
      "userId_problemId_remap file saved successfully\n"
     ]
    }
   ],
   "source": [
    "remap_user_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c669747",
   "metadata": {},
   "source": [
    "### train data, test data 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6caf504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_data():\n",
    "    user_problem_remap_df = pd.read_csv(\"./data/userId_problemId_remap.csv\").loc[:, ['userId', 'problemId']]\n",
    "    train_data_df = pd.DataFrame({'userId': [], 'problemId': []}, dtype=int)\n",
    "    test_data_df = pd.DataFrame({'userId': [], 'problemId': []}, dtype=int)\n",
    "    \n",
    "    for user_id in user_problem_remap_df['userId'].unique():\n",
    "        problem_list = user_problem_remap_df[user_problem_remap_df['userId'] == user_id]['problemId'].tolist()\n",
    "        train_problem_list = random.sample(problem_list, int(len(problem_list) * 0.8))\n",
    "        test_problem_list = list(set(problem_list) - set(train_problem_list))\n",
    "        \n",
    "        new_train_data_df = pd.DataFrame({'userId': [user_id], 'problemId': [train_problem_list]})\n",
    "        new_test_data_df = pd.DataFrame({'userId': [user_id], 'problemId': [test_problem_list]})\n",
    "        \n",
    "        train_data_df = pd.concat((train_data_df, new_train_data_df))\n",
    "        test_data_df = pd.concat((test_data_df, new_test_data_df))\n",
    "    \n",
    "    with open(\"./data/train.txt\", \"w\") as f:\n",
    "        for user_id in sorted(train_data_df['userId'].tolist()):\n",
    "            problem_list = [*map(int, train_data_df[train_data_df['userId'] == user_id]['problemId'].tolist()[0])]\n",
    "            f.write(str(user_id) + ' ' + ' '.join(map(str, problem_list)) + '\\n')\n",
    "    \n",
    "    with open(\"./data/test.txt\", \"w\") as f:\n",
    "        for user_id in sorted(test_data_df['userId'].tolist()):\n",
    "            problem_list = [*map(int, test_data_df[test_data_df['userId'] == user_id]['problemId'].tolist()[0])]\n",
    "            f.write(str(user_id) + ' ' + ' '.join(map(str, problem_list)) + '\\n')\n",
    "        \n",
    "    print('train.txt, test.txt saved successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9756e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.txt, test.txt saved successfully\n"
     ]
    }
   ],
   "source": [
    "making_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f298f2e1",
   "metadata": {},
   "source": [
    "# NGCF 모델 관련 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327f568a",
   "metadata": {},
   "source": [
    "### 전역변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03b3addf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_items, test_set = {}, {}\n",
    "matrix = None\n",
    "exist_users = []\n",
    "global_epoch_value = 0\n",
    "result_arr = []\n",
    "\n",
    "n_users, n_items, n_train, n_test = 0, 0, 0, 0\n",
    "total_epoch = 20\n",
    "embed_size = 64\n",
    "batch_size = 1024\n",
    "layer_size = [64, 64, 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d62c2f2",
   "metadata": {},
   "source": [
    "### NGCF 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6ac9aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGCF(nn.Module):\n",
    "    def __init__(self, n_user, n_item, norm_adj, emb_size, batch_size, layer_size):\n",
    "        super(NGCF, self).__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.norm_adj = norm_adj\n",
    "        self.emb_size = emb_size\n",
    "        self.batch_size = batch_size\n",
    "        self.layer_size = layer_size\n",
    "\n",
    "        self.embedding_dict, self.weight_dict = self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        embedding_dict = nn.ParameterDict({\n",
    "            'user_emb': nn.Parameter(nn.init.xavier_uniform_(torch.empty(self.n_user, self.emb_size))),\n",
    "            'item_emb': nn.Parameter(nn.init.xavier_uniform_(torch.empty(self.n_item, self.emb_size)))\n",
    "        })\n",
    "\n",
    "        weight_dict = nn.ParameterDict()\n",
    "        layers = [self.emb_size] + self.layer_size\n",
    "        for i in range(len(self.layer_size)):\n",
    "            weight_dict['W_gc_%d' % i] = nn.Parameter(nn.init.xavier_uniform_(torch.empty(layers[i], layers[i + 1])))\n",
    "            weight_dict['W_bi_%d' % i] = nn.Parameter(nn.init.xavier_uniform_(torch.empty(layers[i], layers[i + 1])))\n",
    "\n",
    "        return embedding_dict, weight_dict\n",
    "\n",
    "    def rating(self, u_g_embeddings, pos_i_g_embeddings):\n",
    "        return torch.matmul(u_g_embeddings, pos_i_g_embeddings.t())\n",
    "\n",
    "    def forward(self, users, pos_items, neg_items):\n",
    "        ego_embeddings = torch.cat([self.embedding_dict['user_emb'], self.embedding_dict['item_emb']], 0)\n",
    "        all_embeddings = [ego_embeddings]\n",
    "\n",
    "        for i in range(len(self.layer_size)):\n",
    "            sum_embeddings = torch.matmul(ego_embeddings, self.weight_dict['W_gc_%d' % i])\n",
    "            bi_embeddings = torch.matmul(ego_embeddings.T, self.norm_adj)\n",
    "            bi_embeddings = torch.matmul(bi_embeddings.T, self.weight_dict['W_gc_%d' % i])\n",
    "\n",
    "            ego_embeddings = nn.LeakyReLU(negative_slope=0.2)(sum_embeddings + bi_embeddings)\n",
    "            norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\n",
    "            all_embeddings += [norm_embeddings]\n",
    "\n",
    "        all_embeddings = torch.cat(all_embeddings, 1)\n",
    "        u_g_embeddings = all_embeddings[:self.n_user, :]\n",
    "        i_g_embeddings = all_embeddings[self.n_user:, :]\n",
    "\n",
    "        u_g_embeddings = u_g_embeddings[users, :]\n",
    "        pos_i_g_embeddings = i_g_embeddings[pos_items, :]\n",
    "        neg_i_g_embeddings = i_g_embeddings[neg_items, :]\n",
    "\n",
    "        return u_g_embeddings, pos_i_g_embeddings, neg_i_g_embeddings\n",
    "\n",
    "    def BPR_Loss(self, users, pos_items, neg_items):\n",
    "        pos_scores = torch.sum(torch.mul(users, pos_items), axis=1)\n",
    "        neg_scores = torch.sum(torch.mul(users, neg_items), axis=1)\n",
    "\n",
    "        maxi = nn.LogSigmoid()(pos_scores - neg_scores)\n",
    "\n",
    "        mf_loss = -1 * torch.mean(maxi)\n",
    "\n",
    "        regularizer = (torch.norm(users) ** 2 + torch.norm(pos_items) ** 2 + torch.norm(neg_items) ** 2) / 2\n",
    "        decay = 1e-5\n",
    "        emb_loss = decay * regularizer / self.batch_size\n",
    "\n",
    "        return mf_loss + emb_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47627e9",
   "metadata": {},
   "source": [
    "### 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aab25ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load():\n",
    "    global n_users, n_items, n_train, n_test, matrix\n",
    "\n",
    "    train_file = './data/train.txt'\n",
    "    test_file = './data/test.txt'\n",
    "\n",
    "    with open(train_file) as f:\n",
    "        for line in f.readlines():\n",
    "            x = line.strip().split()\n",
    "\n",
    "            user_id = int(x[0])\n",
    "            exist_users.append(user_id)\n",
    "            n_users = max(n_users, user_id)\n",
    "\n",
    "            items = [*map(int, x[1:])]\n",
    "            n_items = max(n_items, max(items))\n",
    "\n",
    "            n_train += len(items)\n",
    "\n",
    "    with open(test_file) as f:\n",
    "        for line in f.readlines():\n",
    "            x = line.strip().split()\n",
    "\n",
    "            items = [*map(int, x[1:])]\n",
    "            n_items = max(n_items, max(items))\n",
    "            n_test += len(items)\n",
    "\n",
    "    n_users += 1\n",
    "    n_items += 1\n",
    "\n",
    "    matrix = torch.zeros((n_users, n_items))\n",
    "\n",
    "    with open(train_file) as f_train:\n",
    "        with open(test_file) as f_test:\n",
    "            for line in f_train.readlines():\n",
    "                x = line.strip().split()\n",
    "                items = [*map(int, x)]\n",
    "                user_id, t_items = items[0], items[1:]\n",
    "                for item in t_items:\n",
    "                    matrix[user_id, item] = 1\n",
    "                train_items[user_id] = t_items\n",
    "            for line in f_test.readlines():\n",
    "                x = line.strip().split()\n",
    "                items = [*map(int, x)]\n",
    "                user_id, t_items = items[0], items[1:]\n",
    "                test_set[user_id] = t_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06166349",
   "metadata": {},
   "source": [
    "### 학습할 때 필요한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7787dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample():\n",
    "    if batch_size <= n_users:\n",
    "        users = random.sample(exist_users, batch_size)\n",
    "    else:\n",
    "        users = [random.choice(exist_users) for i in range(batch_size)]\n",
    "\n",
    "    pos_items, neg_items = [], []\n",
    "    for user in users:\n",
    "        pos_item_list = train_items[user]\n",
    "        pos_batch = pos_item_list[np.random.randint(0, len(pos_item_list))]\n",
    "        pos_items += [pos_batch]\n",
    "\n",
    "        while 1:\n",
    "            neg_item_list = train_items[user]\n",
    "            neg_id = np.random.randint(0, n_items)\n",
    "            if neg_id not in train_items[user] and neg_id not in neg_item_list:\n",
    "                neg_items.append(neg_id)\n",
    "                break\n",
    "    return users, pos_items, neg_items\n",
    "\n",
    "\n",
    "def get_norm_adj():\n",
    "    adj_mat = torch.zeros([n_users + n_items, n_users + n_items])\n",
    "    adj_mat[:n_users, n_users:] = matrix\n",
    "    adj_mat[n_users:, :n_users] = matrix.T\n",
    "    rowsum = np.array(adj_mat.sum(1))\n",
    "    d_inv = rowsum.copy()\n",
    "    for i in range(rowsum.size):\n",
    "        if d_inv[i] != 0:\n",
    "            d_inv[i] = 1 / d_inv[i]\n",
    "    d_mat_inv = np.diag(d_inv)\n",
    "\n",
    "    return torch.from_numpy(d_mat_inv.dot(adj_mat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab39610c",
   "metadata": {},
   "source": [
    "### 평가할 때 필요한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fcedfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranklist_by_heapq(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "    K_max = Ks\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    if global_epoch_value == total_epoch - 1:\n",
    "        result_arr.append(K_max_item_score)\n",
    "    r = []\n",
    "    for val in K_max_item_score:\n",
    "        if val in user_pos_test:\n",
    "            r += [1]\n",
    "        else:\n",
    "            r += [0]\n",
    "    return r\n",
    "\n",
    "\n",
    "def get_performance(r, Ks):\n",
    "    return np.mean(np.asarray(r)[:Ks])\n",
    "\n",
    "\n",
    "def test_one_user(x, y):\n",
    "    rating = x\n",
    "    user = y\n",
    "    if len(train_items[user]) == 0:\n",
    "        training_items = []\n",
    "    else:\n",
    "        training_items = train_items[user]\n",
    "\n",
    "    user_pos_test = test_set[user]\n",
    "    all_items = set(range(n_items))\n",
    "    test_items = list(all_items - set(training_items))\n",
    "    r = ranklist_by_heapq(user_pos_test, test_items, rating, 100)\n",
    "\n",
    "    return get_performance(r, 100)\n",
    "\n",
    "\n",
    "def test(model, users_to_test):\n",
    "    result = 0\n",
    "    u_batch_size = batch_size * 2\n",
    "\n",
    "    test_users = users_to_test\n",
    "    n_test_users = len(test_users)\n",
    "    n_users_batchs = n_test_users // u_batch_size + 1\n",
    "\n",
    "    for u_batch_id in range(n_users_batchs):\n",
    "        start = u_batch_id * u_batch_size\n",
    "        end = (u_batch_id + 1) * u_batch_size\n",
    "\n",
    "        user_batch = test_users[start:end]\n",
    "        item_batch = range(n_items)\n",
    "        u_g_embedding, pos_i_g_embedding, _ = model(user_batch, item_batch, [])\n",
    "        rate_batch = model.rating(u_g_embedding, pos_i_g_embedding).detach()\n",
    "\n",
    "        for i in range(len(user_batch)):\n",
    "            result += test_one_user(rate_batch.numpy()[i], user_batch[i])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b585e4a",
   "metadata": {},
   "source": [
    "### 학습 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24b22c94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_result(epoch_value = 200, flag = True):\n",
    "    '''\n",
    "    :param epoch_value: epoch 값\n",
    "    :param flag: True면 이미 저장된 값을 return, False면 새로 학습시켜서 return\n",
    "    :return: 각 유저에게 유사도가 제일 높은 순서로 문제 번호를 반환하는 2차원 리스트\n",
    "    '''\n",
    "    global n_users, n_items, n_train, n_test, total_epoch, embed_size, batch_size, global_epoch_value\n",
    "    n_users, n_items, n_train, n_test = 0, 0, 0, 0\n",
    "    data_load()\n",
    "    norm_adj = get_norm_adj()\n",
    "    total_epoch = epoch_value\n",
    "    embed_size = 64\n",
    "    batch_size = 1024\n",
    "    layer_size = [64, 64, 64]\n",
    "    result_arr = []\n",
    "\n",
    "    if flag and os.path.isfile('./data/rank.pkl'):\n",
    "        print('file exist')\n",
    "        with open('filename.pkl', 'rb') as f:\n",
    "            result_arr = pickle.load(f)\n",
    "        return result_arr\n",
    "    else:\n",
    "        print('file does not exist')\n",
    "\n",
    "    model = NGCF(n_users, n_items, norm_adj, embed_size, batch_size, layer_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(total_epoch):\n",
    "        global_epoch_value = epoch\n",
    "        time1 = time.time()\n",
    "\n",
    "        loss = 0\n",
    "        n_batch = n_train // batch_size + 1\n",
    "        for idx in range(n_batch):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            users, pos_items, neg_items = sample()\n",
    "            u_g_embedding, pos_i_g_embedding, neg_i_g_embedding = model(users, pos_items, neg_items)\n",
    "            batch_loss = model.BPR_Loss(u_g_embedding, pos_i_g_embedding, neg_i_g_embedding)\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss += batch_loss\n",
    "\n",
    "        time2 = time.time()\n",
    "        print(f'Epoch: {epoch}, loss: {loss}, time: {int(time2 - time1)}')\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            users_to_test = list(test_set.keys())\n",
    "            ret = test(model, users_to_test)\n",
    "            print(f'Precision: {ret}')\n",
    "    \n",
    "    with open('./data/rank.pkl', 'wb') as f:\n",
    "        pickle.dump(result_arr, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return result_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f9f319",
   "metadata": {},
   "source": [
    "### 학습된 결과 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77771f20",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16460\\2301351938.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16460\\27746049.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(epoch_value, flag)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mn_users\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_epoch_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mn_users\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mdata_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mnorm_adj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_norm_adj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtotal_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16460\\1493494473.py\u001b[0m in \u001b[0;36mdata_load\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtest_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./data/test.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/train.txt'"
     ]
    }
   ],
   "source": [
    "print(len(get_result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc81c8d",
   "metadata": {},
   "source": [
    "## 결과 출력\n",
    "\n",
    "1. 유저가 이미 풀은 문제는 제외\n",
    "2. 유저 id와 문제 id가 매핑되어 있으므로 원래대로 바꾸어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d80d361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
